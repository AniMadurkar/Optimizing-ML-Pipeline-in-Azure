# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset contains data about direct marketing campaigns (phone calls) from a Portugese banking institution. We will be using this dataset to determine the success of Bank Telemarketing, by seeking to predict whether the client will subscribe a term deposit (target variable y). The performance metric we were trying to optimize/maximize was accuracy, and based on this the best performing model was the AutoML Voting Ensemble model which yielded .917 accuracy in comparison to the top hyperdrive logistic regression model which yielded .909 accuracy. This is interesting as we only tune two hyperparameters for a relatively simple model (logistic regression) and the automl model only seemed to yield a marginal improvement. Additionally, accuracy is a pretty simplistic measure to assess model performance on, we should re-evaluate the use case to see if this makes the most sense.

## Scikit-learn Pipeline
The pipeline architecture involves an SKLearn estimator that gets the output of a train.py file. This train.py file reads in bankmarketing dataset, conducts feature engineering through OneHotEncoding the categorical variables and creating numerical features of the binary categorical variables in the dataset, and then splits the data into train/test splits of the features and target variable. The train.py file then takes the hyperparameter arguments passed in for C (the inverse of regularization strength) and max_iter (maximum iterations until convergence) and runs a Logistic Regression model. This model is scored for accuracy and then saved out to a file path. For hyperparameter tuning, we assess these values for C: .001, .01, .1, .3, .5, 1.0. and these for max_iter: 50, 100, 150, 200 through a random parameter sampler which selects a choice of one from each hyperparameter at random. We also have a bandit policy for termination criteria with a slack factor of .1 as the ratio threshold for the distance from the best performing experiment run. The Logistic Regression classification model is a generalized linear model that uses a logistic function to model a binary dependent variable. This pipeline is set up to maximize accuracy.

The benefits of the random parameter sampler are primarily for computational cost reasons, and not as much for performance. It will randomly choose combinations of hyperparameters and supports early termination of low performance runs, which is good typically for an initial search. I would increase the runs and conduct the Bayesian sampling next as it will tune hyperparameters given the priors from previous runs. This is typically more costly and time intensive, though. 

The benefits of the early stopping policy I chose is that it specifically looks at the distance between the model performance evaluation metric from the best model based on some percentage and this is beneficial as we currently only have one performance metric listed. This will clearly help us find the run/model with top performance based on the metric we chose. If we had multiple, the Median policy would likely be beneficial as it gets the median of the running averages and concurrently stops those with values lower than the median.

## AutoML
The AutoML models were 23 different models and they all were either a Gradient Boosted Decision Tree, Random Forest, or some other Tree Ensemble model. The best one was the Voting Ensemble. Ensemble modeling is a process where multiple diverse models are created with the aim to aggregate the predictions of each base model, resulting in one final prediction for the unseen data that is usually more accurate than the each one returned by each single model. Basically, ensembling strategies reduce the generalization error of the prediction. Voting Ensemble leverages soft-voting which uses weighted averages. The Voting Ensemble model beats out the next best by only .002 accuracy though, which is quite the marginal amount and can even be chalked up to stochasticity.

## Pipeline comparison
The AutoML VotingEnsemble model beats the Logistic Regression model by about .06 accuracy, which is also an incredibly marginal amount. I believe this kind of difference was likely due to model architecture as the VotingEnsemble model runs a lot of diverse models and then averages the results which has commonly yielded strong results. Logistic Regression model only runs for two hyperparameters and just selects the highest value; this quite simplistic architecture does well but is bound to get beaten by tree-based ensemble models.

## Future work
Even with the performance difference, this difference is small and that is likely because we don't have that many informative features & have class imbalance in the dataset. Feature engineering is bound to be able to extract the last bit of performance, and also handling the classification imbalance in the dataset. We don't have an equal, or relatively equal, number of values for our binary classification target values and this poses problem if we don't balance the dataset in the train.py file. Implementing SMOTE and creating stronger features is likely to improve performance quite a bit, and I would wager AutoML would outperform by a large margin after these improvements, especially because of the diversity and combination of the models leveraged.
